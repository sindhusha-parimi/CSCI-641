# -*- coding: utf-8 -*-
"""Clinical Trials ULMFiT and USE Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oh4EuSyUADh6DlgheVTwXiyWGITj9Xiu

Implementing FastAIâ€™s ULMFiT to make a multi-class text classifier which we then use to predict the Conditions that a research is working on.
"""

#Importing clinical trials data.
import pandas as pd

#We processed the clinical trials dataset and extracted the Title column and Conditions labels into a CSV file to use it for the model building
mydata = pd.read_csv('Clinical_data.csv')
mydata

"""In the code block below we remove those rows whose classes have insufficient numbers to be able to split by SKlearns split function validly."""

a = mydata[mydata.Class != 1302]
b = a[a.Class != 1303]
c = b[b.Class != 1402]
d = c[c.Class != 1602]
e = d[d.Class != 1608]
f = e[e.Class != 10]
g = f[f.Class != 16]
h = g[g.Class != 912]
h

k = h.drop('Unnamed: 0',axis =1)
k

"""Here we split the data into train and test data sets"""

from sklearn.model_selection import train_test_split
df_trn, df_val = train_test_split(mydata, stratify = mydata['label'], test_size = 0.3)
df_trn.shape, df_val.shape

from fastai.text import *
# Language model data
data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = "")
# Classifier model data
data_clas = TextClasDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = "", vocab=data_lm.train_ds.vocab, bs=32)

#Saving the language model and classifier model to load them again
data_lm.save('data_lm_export.pkl')
data_clas.save('data_clas_export.pkl')
data_lm = load_data('./', 'data_lm_export.pkl')
data_clas = load_data('./', 'data_clas_export.pkl', bs=64)

#build model
learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)

#finding optimal learning rate

learn.lr_find()
learn.recorder.plot(suggestion=True)
min_grad_lr = learn.recorder.min_grad_lr

#training the model for 2 epochs
learn.fit_one_cycle(2, min_grad_lr)

#unfreezing all layers

learn.unfreeze()
learn.fit_one_cycle(2, 1e-3)



learn.save_encoder('ft_enc')

#passing the input data and drop rate
learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)

learn.load_encoder('ft_enc')

#Finding optimal learning rate when the layers are unfreezed
learn.lr_find()
learn.recorder.plot(suggestion=True)
min_grad_lr = learn.recorder.min_grad_lr

learn.fit_one_cycle(2, min_grad_lr)

learn.recorder.plot_losses()

learn.freeze_to(-2)
learn.fit_one_cycle(4, slice(5e-3, 2e-3), moms=(0.8,0.7))

learn.recorder.plot_losses()

learn.unfreeze()
learn.fit_one_cycle(4, slice(2e-3/100, 2e-3), moms=(0.8,0.7))

#predictions on unseen test data
preds,y,losses = learn.get_preds(with_loss=True)
interp = ClassificationInterpretation(learn, preds, y, losses)
#plotting confusion matrix
interp.plot_confusion_matrix()

learn.predict("Acute Effects of Cannabis on Cognition and Mobility in Older HIV-infected and Uninfected Women")



"""***SECOND MODEL***

**Prediction using Universal Sentence Encoder**
"""

# Install the latest Tensorflow version.
!pip3 install --quiet "tensorflow>=1.7"
# Install TF-Hub.
!pip3 install --quiet tensorflow-hub
!pip3 install seaborn

#importing required libraries
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import keras.layers as layers
from keras.models import Model
from keras import backend as K
np.random.seed(10)

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3" 
embed = hub.Module(module_url)

#importing the processed data
mydata = pd.read_csv('not_final3.csv')
mydata = mydata.drop('Unnamed: 0',axis =1)
mydata

#split the data into train and test
from sklearn.model_selection import train_test_split 

X_train , X_test  = train_test_split(mydata , stratify=mydata.label,test_size = .20 , )

#Explicitly cast the input as a string
def UniversalEmbedding(x):
    return embed(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=True)["default"]

#Building the model with 2 layers, relu and softmax activation functions

input_text = layers.Input(shape=(1,), dtype=tf.string)
embedding = layers.Lambda(UniversalEmbedding, output_shape=(512,))(input_text)
dense = layers.Dense(256, activation='relu')(embedding)
pred = layers.Dense(17, activation='softmax')(dense)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

#1-hot encoding our train classes
train_text = X_train['text'].tolist()
train_text = np.array(train_text, dtype=object)[:, np.newaxis]

train_label = np.asarray(pd.get_dummies(X_train['label']), dtype = np.int8)

#1-hot encoding our test classes
test_text = X_test['text'].tolist()
test_text = np.array(test_text, dtype=object)[:, np.newaxis]
test_label = np.asarray(pd.get_dummies(X_test['label']), dtype = np.int8)

#This only train and save our Keras layers not the embed module' weights.
with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  history = model.fit(train_text, 
            train_label,
            validation_data=(test_text, test_label),
            epochs=10,
            batch_size=32)
  model.save_weights('./model.h5')

#VISUALIZATION OF ACCURACY AFTER AUGMENTATION
import matplotlib.pyplot as plt

def acc_loss_plot(history, epochs):
    epochs = 10
    acc = history.history['acc']
    val_acc = history.history['val_acc']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(epochs)
    #training and validation accuracy curve

    plt.figure(figsize=(14, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    #training and validation loss curve
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()
acc_loss_plot(history , 20)

new_text = ["Diet-induced Modification of Sweet Taste Perception and Preference", "Endometrial Biopsy Biomarkers for Prediction of IVF Outcomes", "Behavioral Pharmacology of Cannabis and Nicotine"]
new_text = np.array(new_text, dtype=object)[:, np.newaxis]
with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  model.load_weights('./model.h5')  
  predicts = model.predict(new_text, batch_size=32)

#predictions on unseen test data

categories = X_train['label'].tolist()
predict_logits = predicts.argmax(axis=1)
predict_labels = [categories[logit] for logit in predict_logits]
predict_labels

